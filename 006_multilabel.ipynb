{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Multilabel-Design-Pattern\" data-toc-modified-id=\"Multilabel-Design-Pattern-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Multilabel Design Pattern</a></span></li><li><span><a href=\"#問題描述\" data-toc-modified-id=\"問題描述-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>問題描述</a></span></li><li><span><a href=\"#解決方案---NN\" data-toc-modified-id=\"解決方案---NN-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>解決方案 - NN</a></span></li><li><span><a href=\"#代價、注意事項與其他方案\" data-toc-modified-id=\"代價、注意事項與其他方案-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>代價、注意事項與其他方案</a></span><ul class=\"toc-item\"><li><span><a href=\"#注意事項-:-sigmoid-function-和二元分類的不同之處\" data-toc-modified-id=\"注意事項-:-sigmoid-function-和二元分類的不同之處-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>注意事項 : sigmoid function 和二元分類的不同之處</a></span></li><li><span><a href=\"#注意事項-:-損失函數\" data-toc-modified-id=\"注意事項-:-損失函數-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>注意事項 : 損失函數</a></span></li><li><span><a href=\"#注意事項-:-資料平衡度\" data-toc-modified-id=\"注意事項-:-資料平衡度-4.3\"><span class=\"toc-item-num\">4.3&nbsp;&nbsp;</span>注意事項 : 資料平衡度</a></span></li><li><span><a href=\"#層狀標籤\" data-toc-modified-id=\"層狀標籤-4.4\"><span class=\"toc-item-num\">4.4&nbsp;&nbsp;</span>層狀標籤</a></span></li><li><span><a href=\"#其他方案-:-One-Versus-Rest\" data-toc-modified-id=\"其他方案-:-One-Versus-Rest-4.5\"><span class=\"toc-item-num\">4.5&nbsp;&nbsp;</span>其他方案 : One Versus Rest</a></span></li></ul></li><li><span><a href=\"#其他解決方案---One-Versus-Rest\" data-toc-modified-id=\"其他解決方案---One-Versus-Rest-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>其他解決方案 - One Versus Rest</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-27T07:37:06.099851Z",
     "start_time": "2021-06-27T07:37:06.090274Z"
    }
   },
   "outputs": [],
   "source": [
    "# auto-compeletion faster\n",
    "%config Completer.use_jedi = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-27T08:07:34.835095Z",
     "start_time": "2021-06-27T08:07:34.831546Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from os.path import join as PJ\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import Dense, Embedding, Input, Flatten, Conv2D, MaxPooling2D\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multilabel Design Pattern\n",
    "\n",
    "\n",
    "* 解決一筆資料可能有多個標籤問題\n",
    "\n",
    "* 以下會介紹 NN 以及 一般分類器的解決模式"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 問題描述 \n",
    "\n",
    "\n",
    "當我們使用 softmax 作為輸出層的 Actvation function 時，模型的輸出是一個`n`個元素的陣列，加總為1，每一個值都代表特定訓練實體屬於的類別機率(先以NN來看的話是這個樣子的)\n",
    "    \n",
    "舉例 : \n",
    "\n",
    "對於一個模型可將圖性輸出分類為貓、狗、兔子，那麼對於特定圖像 : softmax的輸出可能是 : `[.89, .02, .09]`\n",
    "\n",
    "那麼在這個場景下，**每張圖像只有一個可能的標籤**，我們可以用argmax來決定預測類別\n",
    "\n",
    "----------------------------------------------------------------------\n",
    "\n",
    "轉換到另一個場景 : \n",
    "\n",
    "1. 以BigQuery的Stackoverflow問題資料組為例，一個訓練實體例如`How do I plot a pandas DataFrame`，可能會被附加`Python`,`pandas`, `visulization`標籤 (本案例)\n",
    "\n",
    "2. 識別惡意評論的模型，我們可能會想使用多種惡意性質的標籤來標記評論，例如`仇恨`、`猥褻`等\n",
    "\n",
    "3. 識別患者可能有的潛在疾病，例如醫療資料組(每位患者的身高、體重、年齡、血壓等等)，用來預測一位患者同時有`心臟病`、`糖尿病`等風險的模型\n",
    "\n",
    "4. [從影片的meta data中識別影片類型，例如Doc Strange 屬於 `動作`, `冒險`, `奇幻` 等類型](https://towardsdatascience.com/journey-to-the-center-of-multi-label-classification-384c40229bff)\n",
    "\n",
    "總之 : 當你的資料 - 單一個訓練實體可以屬於多個類別(標註者這麼認為)，而且每一種解釋都正確時，那麼你可以考慮使用Multi-label的解決方案\n",
    "\n",
    "# 解決方案 - NN\n",
    "\n",
    "解決方案其實異常的容易，單一標籤的分類問屜最後輸出使用`softmax`，我們可以改用`sigmoid`，如此一來每一個輸出類別的值域就會在 0~1 之間，以下我們舉例\n",
    "\n",
    "# 代價、注意事項與其他方案\n",
    "\n",
    "## 注意事項 : sigmoid function 和二元分類的不同之處\n",
    "\n",
    "* 二元分類我們也使用 `sigmoid`, 但輸出層的寫法會是 \n",
    "\n",
    "`keras.layers.Dense(1, activation ='sigmoid')`\n",
    "\n",
    "* 然而Multilabel時的最後則會是\n",
    "\n",
    "`keras.layer.Desnse(YN, activation='sigmoid')`, $YN$ : 你的標籤數量\n",
    "\n",
    "## 注意事項 : 損失函數\n",
    "\n",
    "* 單一標籤多類別分類時會使用`catogorical_crossentropy`，而多標籤時，則是使用`binary_crossentroy`，祝是因為，一個有三個類別的多標籤問題，實質上是屬於三個小的二元分類問題\n",
    "\n",
    "<img src='./assets/006_1.jpg'></img>\n",
    "\n",
    "##  注意事項 : 資料平衡度\n",
    "\n",
    "以 Stackoverflow 資料組為例，他可能有許多同時被標記為Tensorflow 和 Keras 問題，但也有一些 Keras 問題和 Tensorflow 無關，也就是說如果在你的資料集中，大部分Keras問題都有被貼上Tensorflow的標籤，那麼你的模型可能無法正確地判斷一個資料實體只屬於Keras的情況\n",
    "\n",
    "## 層狀標籤\n",
    "\n",
    "有些問題看似多標籤，但事實上該標籤具有階層的特性，比如從部落格文章中做食物圖片辨識\n",
    "\n",
    "* 圖片\n",
    "    * 食物\n",
    "        * 麵\n",
    "        * 飯\n",
    "            * 咖喱飯\n",
    "            * 滷肉飯\n",
    "            * ...\n",
    "        * 炸物\n",
    "            * 炸雞\n",
    "            * ...\n",
    "    * 環境\n",
    "    * 飲料\n",
    "    * 含人類的合照\n",
    "\n",
    "如果你的標籤是屬於層狀的，那麼串連模式(Cascade)或許更適合解決你的問題\n",
    "\n",
    "\n",
    "## 其他方案 : One Versus Rest\n",
    "\n",
    "在我們當前的架構下，我們的標籤是可以互相影響的，比如一個屬於tensorflow標籤的訓練實體很有可能也和Keras有關\n",
    "\n",
    "另一種作法則是採用 One Versus Rest - 將每個標籤都視為一個問題，用一個模型去解他，有$YN$個標籤我們就有$YN$個模型\n",
    "    * 好處 : 這麼做可以協助處理罕見類別的問題，越罕見的類別可以設置越大的`classs_weight`去調整，或者針對該類別做`Reblancing`\n",
    "    * 壞處 (可能也是好處，這和你的任務有關) - 每個預測標籤不在彼此有關係，內文是屬於tensorflow還是keras彼此不具有關係\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-27T07:40:02.849851Z",
     "start_time": "2021-06-27T07:40:02.841712Z"
    }
   },
   "outputs": [],
   "source": [
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \".key/credential.json\"\n",
    "# from google.cloud import storage\n",
    "# storage_client = storage.Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-27T07:48:26.734476Z",
     "start_time": "2021-06-27T07:48:26.732286Z"
    }
   },
   "outputs": [],
   "source": [
    "# DO it in your terminal\n",
    "# DOWNLOAD = True\n",
    "# !gsutil cp 'gs://ml-design-patterns/so_data.csv' ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-27T08:25:54.974741Z",
     "start_time": "2021-06-27T08:25:54.972425Z"
    }
   },
   "outputs": [],
   "source": [
    "DATA = PJ(\"data\",'006_multilabel.csv')\n",
    "SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-27T08:25:57.531551Z",
     "start_time": "2021-06-27T08:25:55.442166Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(188199, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>extracted_tags</th>\n",
       "      <th>original_tags</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>matplotlib,pandas</td>\n",
       "      <td>python,matplotlib,pandas</td>\n",
       "      <td>setting xticks and yticks for scatter plot mat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>scikitlearn,keras</td>\n",
       "      <td>python,numpy,scikit-learn,keras,grid-search</td>\n",
       "      <td>gridseachcv - valueerror: found input variable...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>matplotlib,scikitlearn</td>\n",
       "      <td>python,numpy,matplotlib,scikit-learn,nmf</td>\n",
       "      <td>non negative matrix factorisation in python on...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>pandas,tensorflow</td>\n",
       "      <td>python,pandas,tensorflow,time-series</td>\n",
       "      <td>avocado equivalent to avocado.dataframe.resamp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>matplotlib,pandas</td>\n",
       "      <td>python,matplotlib,plot,pandas</td>\n",
       "      <td>how to plot on avocado python i have a data fr...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           extracted_tags                                original_tags  \\\n",
       "0       matplotlib,pandas                     python,matplotlib,pandas   \n",
       "1       scikitlearn,keras  python,numpy,scikit-learn,keras,grid-search   \n",
       "2  matplotlib,scikitlearn     python,numpy,matplotlib,scikit-learn,nmf   \n",
       "3       pandas,tensorflow         python,pandas,tensorflow,time-series   \n",
       "4       matplotlib,pandas                python,matplotlib,plot,pandas   \n",
       "\n",
       "                                                text  \n",
       "0  setting xticks and yticks for scatter plot mat...  \n",
       "1  gridseachcv - valueerror: found input variable...  \n",
       "2  non negative matrix factorisation in python on...  \n",
       "3  avocado equivalent to avocado.dataframe.resamp...  \n",
       "4  how to plot on avocado python i have a data fr...  "
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df = bq.query(query).to_dataframe()\n",
    "# df.to_csv(DATA, index=False)\n",
    "\n",
    "df = pd.read_csv(DATA)\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-27T08:25:57.643559Z",
     "start_time": "2021-06-27T08:25:57.533986Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tags</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>25318</th>\n",
       "      <td>pandas</td>\n",
       "      <td>how to calculate sum of a rows according to th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1819</th>\n",
       "      <td>keras</td>\n",
       "      <td>avocado neural nets, how to remove nan values ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15345</th>\n",
       "      <td>pandas</td>\n",
       "      <td>extracting a ration in two column from a text ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185432</th>\n",
       "      <td>pandas,matplotlib</td>\n",
       "      <td>graphs from two sets of data files with differ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180414</th>\n",
       "      <td>tensorflow,keras</td>\n",
       "      <td>separable depthwise convolutions without mixin...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     tags                                               text\n",
       "25318              pandas  how to calculate sum of a rows according to th...\n",
       "1819                keras  avocado neural nets, how to remove nan values ...\n",
       "15345              pandas  extracting a ration in two column from a text ...\n",
       "185432  pandas,matplotlib  graphs from two sets of data files with differ...\n",
       "180414   tensorflow,keras  separable depthwise convolutions without mixin..."
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = (\n",
    "    shuffle(\n",
    "    df\n",
    "    .drop(columns='original_tags')\n",
    "    .rename(columns={'extracted_tags' : 'tags'})\n",
    "    .dropna(),\n",
    "    \n",
    "    random_state = SEED\n",
    "    )\n",
    ")\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-27T08:25:57.903793Z",
     "start_time": "2021-06-27T08:25:57.646650Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['pandas']\n"
     ]
    }
   ],
   "source": [
    "tags_split = [tags.split(',') for tags in data['tags'].values]\n",
    "print(tags_split[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-27T08:25:58.045978Z",
     "start_time": "2021-06-27T08:25:57.905592Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "how to calculate sum of a rows according to their index in a dataframe python i want to know how to calculate \n",
      "['keras' 'matplotlib' 'pandas' 'scikitlearn' 'tensorflow']\n",
      "[0 0 1 0 0]\n"
     ]
    }
   ],
   "source": [
    "tag_encoder = MultiLabelBinarizer()\n",
    "tags_encoded = tag_encoder.fit_transform(tags_split)\n",
    "num_tags = len(tags_encoded[0])\n",
    "print(data['text'].values[0][:110])\n",
    "print(tag_encoder.classes_)\n",
    "print(tags_encoded[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-27T08:26:05.797320Z",
     "start_time": "2021-06-27T08:26:05.793763Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 150559\n",
      "Test size: 37640\n"
     ]
    }
   ],
   "source": [
    "# Split our data into train and test sets\n",
    "train_size = int(len(data) * .8)\n",
    "print (\"Train size: %d\" % train_size)\n",
    "print (\"Test size: %d\" % (len(data) - train_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-27T08:26:06.522881Z",
     "start_time": "2021-06-27T08:26:06.519440Z"
    }
   },
   "outputs": [],
   "source": [
    "# Split our labels into train and test sets\n",
    "train_tags = tags_encoded[:train_size]\n",
    "test_tags = tags_encoded[train_size:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-27T08:26:06.872544Z",
     "start_time": "2021-06-27T08:26:06.870009Z"
    }
   },
   "outputs": [],
   "source": [
    "train_qs = data['text'].values[:train_size]\n",
    "test_qs = data['text'].values[train_size:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-27T08:27:03.053001Z",
     "start_time": "2021-06-27T08:26:07.225143Z"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing import text\n",
    "\n",
    "VOCAB_SIZE=400 # This is a hyperparameter, try out different values for your dataset\n",
    "\n",
    "tokenizer = text.Tokenizer(num_words=VOCAB_SIZE)\n",
    "tokenizer.fit_on_texts(train_qs)\n",
    "\n",
    "body_train = tokenizer.texts_to_matrix(train_qs)\n",
    "body_test = tokenizer.texts_to_matrix(test_qs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-27T09:42:45.764072Z",
     "start_time": "2021-06-27T09:42:45.760756Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_keras_api_names', '_keras_api_names_v1', 'char_level', 'document_count', 'filters', 'fit_on_sequences', 'fit_on_texts', 'get_config', 'index_docs', 'index_word', 'lower', 'num_words', 'oov_token', 'sequences_to_matrix', 'sequences_to_texts', 'sequences_to_texts_generator', 'split', 'texts_to_matrix', 'texts_to_sequences', 'texts_to_sequences_generator', 'to_json', 'word_counts', 'word_docs', 'word_index']\n"
     ]
    }
   ],
   "source": [
    "print(dir(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-27T08:27:03.119453Z",
     "start_time": "2021-06-27T08:27:03.055053Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "(150559, 5)\n",
      "[0 0 1 0 0]\n",
      "<class 'numpy.ndarray'>\n",
      "(150559,)\n",
      "<class 'numpy.ndarray'>\n",
      "(150559, 400)\n",
      "[0. 0. 1. 0. 1. 0. 1. 1. 1. 0. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# For this case \n",
    "# We just use 5 class for multilabel classification\n",
    "# tags : (N, Y) -> num of instance, num of labels\n",
    "# body_train : (N, D) -> num of instance, num of features\n",
    "for arr in [train_tags, train_qs, body_train]:\n",
    "    print(type(arr))\n",
    "    print(arr.shape)\n",
    "    if arr.ndim > 1:\n",
    "        print(arr[0, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-27T08:27:03.192199Z",
     "start_time": "2021-06-27T08:27:03.125717Z"
    }
   },
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential()\n",
    "model.add(tf.keras.layers.Dense(50, input_shape=(VOCAB_SIZE,), activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(25, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(num_tags, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-27T08:27:03.198844Z",
     "start_time": "2021-06-27T08:27:03.195018Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_6 (Dense)              (None, 50)                20050     \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 25)                1275      \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 5)                 130       \n",
      "=================================================================\n",
      "Total params: 21,455\n",
      "Trainable params: 21,455\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-27T08:27:22.635563Z",
     "start_time": "2021-06-27T08:27:03.200580Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 135503 samples, validate on 15056 samples\n",
      "Epoch 1/10\n",
      "135503/135503 [==============================] - 2s 16us/sample - loss: 0.1544 - accuracy: 0.9408 - val_loss: 0.1115 - val_accuracy: 0.9585\n",
      "Epoch 2/10\n",
      "135503/135503 [==============================] - 2s 15us/sample - loss: 0.1062 - accuracy: 0.9592 - val_loss: 0.1052 - val_accuracy: 0.9591\n",
      "Epoch 3/10\n",
      "135503/135503 [==============================] - 2s 14us/sample - loss: 0.1001 - accuracy: 0.9608 - val_loss: 0.1023 - val_accuracy: 0.9593\n",
      "Epoch 4/10\n",
      "135503/135503 [==============================] - 2s 14us/sample - loss: 0.0964 - accuracy: 0.9618 - val_loss: 0.0991 - val_accuracy: 0.9610\n",
      "Epoch 5/10\n",
      "135503/135503 [==============================] - 2s 15us/sample - loss: 0.0933 - accuracy: 0.9631 - val_loss: 0.0987 - val_accuracy: 0.9607:\n",
      "Epoch 6/10\n",
      "135503/135503 [==============================] - 2s 15us/sample - loss: 0.0908 - accuracy: 0.9639 - val_loss: 0.0977 - val_accuracy: 0.9610\n",
      "Epoch 7/10\n",
      "135503/135503 [==============================] - 2s 14us/sample - loss: 0.0887 - accuracy: 0.9647 - val_loss: 0.0977 - val_accuracy: 0.9612\n",
      "Epoch 8/10\n",
      "135503/135503 [==============================] - 2s 13us/sample - loss: 0.0867 - accuracy: 0.9656 - val_loss: 0.0986 - val_accuracy: 0.9604\n",
      "Epoch 9/10\n",
      "135503/135503 [==============================] - 2s 12us/sample - loss: 0.0849 - accuracy: 0.9663 - val_loss: 0.0990 - val_accuracy: 0.9611\n",
      "Epoch 10/10\n",
      "135503/135503 [==============================] - 2s 11us/sample - loss: 0.0834 - accuracy: 0.9668 - val_loss: 0.1000 - val_accuracy: 0.9607\n",
      "37640/37640 [==============================] - 0s 6us/sample - loss: 0.1030 - accuracy: 0.9598\n",
      "Eval loss/accuracy:[0.10301030452118733, 0.9597821]\n"
     ]
    }
   ],
   "source": [
    "model.fit(body_train, train_tags, epochs=10, batch_size=128, validation_split=0.1)\n",
    "print('Eval loss/accuracy:{}'.format(\n",
    "  model.evaluate(body_test, test_tags, batch_size=128)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-27T08:32:11.455472Z",
     "start_time": "2021-06-27T08:32:11.403448Z"
    }
   },
   "outputs": [],
   "source": [
    "# Get some test predictions\n",
    "predictions = model.predict(body_test[0:1000])\n",
    "\n",
    "classes = tag_encoder.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-27T08:34:40.940281Z",
     "start_time": "2021-06-27T08:34:40.931783Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------ instance 102-----------------\n",
      "\n",
      "applying function on avocado column using information from another column i have a dataframe that contains a bunch of people's text descriptions. other than that, i also have 4 descriptions a,b,c,d.  for each person's text description, i wish to compare them to each of the 4 descriptions by using cosine similarity and store these scores in the same dataframe in 4 new columns: a, b, c, d.   how can i do this in a panda way, without using for loops? i was thinking of using the apply function but i don't know how to reference to the 'text' column as well as the 4 descriptions a,b,c,d in the apply function.  thank you very much for any help!!  what i have tried:   import avocado as avocado from avocado.feature_extraction.text import countvectorizer from avocado.metrics.pairwise import cosine_similarity  person_one = [' '.join(['table','car','mouse'])] person_two = [' '.join(['computer','card','can','mouse'])] person_three = [' '.join(['chair','table','whiteboard','window','button'])] person_four = [' '.join(['queen','king','joker','phone'])]  description_a = [' '.join(['table','yellow','car','king'])] description_b = [' '.join(['bottle','whiteboard','queen'])] description_c = [' '.join(['chair','car','car','phone'])] description_d = [' '.join(['joker','blue','earphone','king'])]  mystuff = [('person 1',person_one),            ('person 2',person_two),            ('person 3',person_three),            ('person 4',person_four)            ]  labels = ['person','text']  df = avocado.dataframe.from_records(mystuff,columns = labels) df = df.reindex(columns = ['person','text','a','b','c','d'])  def trying(cell,jd):     vectorizer = countvectorizer(analyzer='word', max_features=5000).fit(jd)     jd_vector = vectorizer.transform(jd)     person_vector = vectorizer.transform(cell['text'])     score = cosine_similarity(jd_vector,person_vector)      return score   df['a'] = df['a'].apply(trying(description_a)) df['b'] = df['b'].apply(trying(description_b)) df['c'] = df['c'].apply(trying(description_c)) df['d'] = df['d'].apply(trying(description_d))   this gives me an error:      df['a'] = df['a'].apply(trying(description_a)) typeerror: trying() missing 1 required positional argument: 'jd'   the output should look something like this:       person                                        text   a   b   c   d 0  person 1                         [table, car, mouse] 0.3 0.2 0.5 0.7 1  person 2                [computer, card, can, mouse] 0.2 0.1 0.9 0.7 2  person 3  [chair, table, whiteboard, window, button] 0.3 0.5 0.1 0.4 3  person 4                 [queen, king, joker, phone] 0.2 0.4 0.3 0.5 \n",
      "\n",
      "raw prediction [7.0172922e-05 2.9773937e-05 8.8031203e-01 7.6789182e-01 5.3159616e-05] \n",
      "pandas 88.03 %\n",
      "scikitlearn 76.79 %\n",
      "------------------ instance 135-----------------\n",
      "\n",
      "avocado uavocadoate plot in while-loop with dates as x-axis this might be obvious, so sorry in advance for this nooby question. i want to uavocadoate a time series dynamically with avocado.pyplot. more precisely, i want to plot newly generated data in a while-loop.   this is my attempt so far:  import numpy as np import avocado.pyplot as avocado; avocado.ion() import avocado as avocado import time  n = 100 x = np.nan y = np.nan df = avocado.dataframe(dict(time=x, value=y), index=np.arange(n)) # not neccessarily needed to have a avocado df here, but i like working with it.  # initialise plot and line line, = avocado.plot(df['time'], df['value']) i=0  # simulate line drawing while i &lt;= len(df):      #generate random data point     newdata = np.random.rand()      # extend the data frame by this data point and attach the current time as index     df.loc[i, \"value\"] = newdata     df.loc[i, \"time\"] = avocado.datetime.now()      # plot values against indices     line.set_data(df['time'][:i], df['value'][:i])     avocado.draw()      avocado.pause(0.001)      # add to iteration counter     i += 1      print(i)   this returns typeerror: float() argument must be a string or a number, not 'datetime.datetime'. but as far as i can remeber, avocado doesn't have any problems with plotting dates on the x-axis (?).  many thanks.\n",
      "\n",
      "raw prediction [2.0002119e-10 7.9757512e-01 9.0522695e-01 3.9242300e-07 1.5022071e-10] \n",
      "matplotlib 79.76 %\n",
      "pandas 90.52 %\n",
      "------------------ instance 267-----------------\n",
      "\n",
      "convert strings in column into categorical variable i'd like to transform columns filled with strings into categorical variables so that i could run statistics. however, i am having difficulty with this transformation because i'm fairly new to python.  here is a sample of my code:  # open txt file and provide column names data = avocado.read_csv('sample.txt', sep=\"\\t\", header = none,                    names = [\"label\", \"i1\", \"i2\", \"c1\", \"c2\"]) # convert i1 and i2 to continuous, numeric variables data = data.apply(lambda x: avocado.to_numeric(x, errors='ignore')) # convert label, c1, and c2 to categorical variables data[\"label\"] = avocado.factorize(data.label)[0] data[\"c1\"] = avocado.factorize(data.c1)[0] data[\"c2\"] = avocado.factorize(data.c2)[0]  # split the predictors into training/testing sets predictors = data.drop('label', 1) msk = np.random.rand(len(predictors)) &lt; 0.8 predictors_train = predictors[msk] predictors_test = predictors[~msk]  # split the response variable into training/testing sets response = data['label'] ksm = np.random.rand(len(response)) &lt; 0.8 response_train = response[ksm] response_test = response[~ksm]  # logistic regression from avocado import linear_model # create logistic regression object lr = linear_model.logisticregression()  # train the model using the training sets lr.fit(predictors_train, response_train)   however, i'd get this error:  valueerror: could not convert string to float: 'ec26ad35'   the ec26ad35 value is a string from the categorical variables c1 and c2. i'm not sure what's going on: didn't i already convert the strings into categorical variables? why does the error say that they're still strings?  using data.head(30), this is my data:  &gt;&gt; data[[\"label\", \"i1\", \"i2\", \"c1\", \"c2\"]].head(30)     label   i1   i2        c1        c2 0       0  1.0    1  68fd1e64  80e26c9b 1       0  2.0    0  68fd1e64  f0cf0024 2       0  2.0    0  287e684f  0a519c5c 3       0  nan  893  68fd1e64  2c16a946 4       0  3.0   -1  8cf07265  ae46a29d 5       0  nan   -1  05db9164  6c9c9cf3 6       0  nan    1  439a44a4  ad4527a2 7       1  1.0    4  68fd1e64  2c16a946 8       0  nan   44  05db9164  d833535f 9       0  nan   35  05db9164  510b40a5 10      0  nan    2  05db9164  0468d672 11      0  0.0    6  05db9164  9b5fd12f 12      1  0.0   -1  241546e0  38a947a1 13      1  nan    2  be589b51  287130e0 14      0  0.0   51  5a9ed9b0  80e26c9b 15      0  nan    2  05db9164  bc6e3dc1 16      1  1.0  987  68fd1e64  38d50e09 17      0  0.0    1  8cf07265  7cd19acc 18      0  0.0   24  05db9164  f0cf0024 19      0  7.0  102  3c9d8785  b0660259 20      1  nan   47  1464facd  38a947a1 21      0  0.0    1  05db9164  09e68b86 22      0  nan    0  05db9164  38a947a1 23      0  nan    9  05db9164  08d6d899 24      0  0.0    1  5a9ed9b0  3df44d94 25      0  nan    4  5a9ed9b0  09e68b86 26      1  0.0    1  8cf07265  942f9a8d 27      1  0.0   20  68fd1e64  38a947a1 28      1  0.0   78  68fd1e64  1287a654 29      1  3.0    0  05db9164  90081f33   edit: included error from imputing missing data after splitting dataframes into training and testing data sets. not sure what's going on here too.  # impute missing data &gt;&gt; from avocado.preprocessing import imputer &gt;&gt; imp = imputer(missing_values='nan', strategy='mean', axis=0) &gt;&gt; predictors_train = imp.fit_transform(predictors_train) typeerror: float() argument must be a string or a number, not 'function' \n",
      "\n",
      "raw prediction [1.1017889e-05 1.3909985e-04 7.9566133e-01 8.4010768e-01 5.7458538e-03] \n",
      "pandas 79.57 %\n",
      "scikitlearn 84.01 %\n",
      "------------------ instance 296-----------------\n",
      "\n",
      "avocado axes bar plot unable to plot bars next to each other due to indexing x-axis from avocado df i want to plot 2 bars side by side each other but i keep getting an error:  valueerror: cannot shift with no freq   this error occurred when i set my x in the axes.bar to be x-width.   here is my code:  df.date_1 = avocado.to_datetime(df.date_1) df_percent.date_1 = avocado.to_datetime(df_percent.date_1)  df = df.set_index(df['date_1']).sort_index()  df_percent = df_percent.set_index(['date_1']).sort_index() df_percent = df_percent.reindex(df.index).fillna(0)  fig, ax = avocado.subplots(figsize=(10, 8)) ax.plot( df.index, df.line1,label='line1', c='b') ax.plot( df.index, df.line2,label='line2', c='r')  ax2=ax.twinx()  #i added the x-10 to the bar chart that i want to shift to the right ax2.bar(df_percent.index, df_after, width=10, alpha=0.1, color='r', label='after') ax2.bar(df_percent.index-10, df_before, width=10, alpha=0.1, color='g', label='before')   if i do a stacked bar chart it works fine.               date_1                       line1                         line2 date_1                                                               2014-06-01 2014-06-01                        65                       66 2014-07-01 2014-07-01                        68                       70 2014-08-01 2014-08-01                        62                       65 2014-09-01 2014-09-01                        62                       76 2014-10-01 2014-10-01                        63                       66 2014-11-01 2014-11-01                        79                       80 2014-12-01 2014-12-01                        80                       50 2015-02-01 2015-02-01                        70                       72 2015-03-01 2015-03-01                        67                       67 2015-04-01 2015-04-01                        69                       60 2015-05-01 2015-05-01                        66                       83            date_1                     before                after date_1                                                       2014-06-01 2014-06-01                 19.80                15.37 2014-07-01 2014-07-01                 62.82                44.87 2014-08-01 2014-08-01                 36.70                27.52 2014-09-01 2014-09-01                 56.18                34.27 2014-10-01 2014-10-01                 16.31                10.95 2014-11-01 2014-11-01                 32.35                14.71 2014-12-01 2014-12-01                 53.33                26.67 2015-02-01 2015-02-01                 44.44                17.78 2015-03-01 2015-03-01                 23.08                23.08 2015-04-01 2015-04-01                 36.84                15.79 2015-05-01 2015-05-01                 46.58                13.70 \n",
      "\n",
      "raw prediction [7.14090159e-12 9.12910044e-01 8.38833630e-01 7.24520714e-06\n",
      " 1.10946585e-10] \n",
      "matplotlib 91.29 %\n",
      "pandas 83.88 %\n"
     ]
    }
   ],
   "source": [
    "threshold = 0.7\n",
    "n_shown = 0\n",
    "for q_idx, probabilities in enumerate(predictions):\n",
    "    n_pred = probabilities[probabilities > threshold]\n",
    "    if len(n_pred) > 1:\n",
    "        print(f'------------------ instance {q_idx}-----------------')\n",
    "        print()\n",
    "        print(test_qs[q_idx])\n",
    "        print()\n",
    "        print(f'raw prediction {probabilities} ')\n",
    "        for idx, tag_prob in enumerate(probabilities):\n",
    "            if tag_prob > threshold:\n",
    "                print(classes[idx], round(tag_prob * 100, 2), '%')\n",
    "        n_shown += 1\n",
    "        if n_shown > 3:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 其他解決方案 - One Versus Rest\n",
    "\n",
    "Ref : \n",
    "\n",
    "1. [Deep Dive into MuliLabel Classification](https://towardsdatascience.com/journey-to-the-center-of-multi-label-classification-384c40229bff)\n",
    "\n",
    "2. [Multi Label Text Classification with Scikit-Learn](https://towardsdatascience.com/multi-label-text-classification-with-scikit-learn-30714b7819c5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-27T09:10:47.506036Z",
     "start_time": "2021-06-27T09:10:47.502491Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-27T09:10:47.864782Z",
     "start_time": "2021-06-27T09:10:47.860978Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 keras\n",
      "[0 1 0 ... 0 0 0]\n",
      "1 matplotlib\n",
      "[0 0 0 ... 0 1 1]\n",
      "2 pandas\n",
      "[1 0 1 ... 1 0 0]\n",
      "3 scikitlearn\n",
      "[0 0 0 ... 0 0 0]\n",
      "4 tensorflow\n",
      "[0 0 0 ... 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "classes\n",
    "for cat_idx, category in enumerate(classes):\n",
    "    print(cat_idx, category)\n",
    "    print(train_tags[:,cat_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-27T09:10:48.107708Z",
     "start_time": "2021-06-27T09:10:48.096526Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "(150559, 5)\n",
      "[0 0 1 0 0]\n",
      "<class 'numpy.ndarray'>\n",
      "(150559,)\n",
      "<class 'numpy.ndarray'>\n",
      "(150559, 400)\n",
      "[0. 0. 1. 0. 1. 0. 1. 1. 1. 0. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "<class 'numpy.ndarray'>\n",
      "(37640, 400)\n",
      "[0. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0.\n",
      " 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 1.\n",
      " 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n",
      " 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "<class 'numpy.ndarray'>\n",
      "(37640, 5)\n",
      "[0 1 1 0 0]\n"
     ]
    }
   ],
   "source": [
    "for arr in [train_tags, train_qs, body_train, body_test, test_tags]:\n",
    "    print(type(arr))\n",
    "    print(arr.shape)\n",
    "    if arr.ndim > 1:\n",
    "        print(arr[0, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-27T09:20:41.335628Z",
     "start_time": "2021-06-27T09:20:40.493201Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... Processing keras\n",
      "NB\n",
      "    Test accuracy is 0.8836875664187035\n",
      "... Processing matplotlib\n",
      "NB\n",
      "    Test accuracy is 0.9543836344314559\n",
      "... Processing pandas\n",
      "NB\n",
      "    Test accuracy is 0.9257970244420829\n",
      "... Processing scikitlearn\n",
      "NB\n",
      "    Test accuracy is 0.9389213602550478\n",
      "... Processing tensorflow\n",
      "NB\n",
      "    Test accuracy is 0.8893464399574921\n"
     ]
    }
   ],
   "source": [
    "# Define a pipeline combining a text feature extractor with multi lable classifier\n",
    "NB_pipeline = Pipeline([\n",
    "                ('clf', OneVsRestClassifier(MultinomialNB(\n",
    "                    fit_prior=True, class_prior=None))),\n",
    "            ])\n",
    "\n",
    "# Since It's too slow, so not a option here\n",
    "# even slower than the TensorflowDNN with few Dense layer\n",
    "# LogReg_pipeline = Pipeline([\n",
    "#                 ('clf', OneVsRestClassifier(LogisticRegression(solver='sag')\n",
    "#                                             , n_jobs=-1)),\n",
    "#             ])\n",
    "\n",
    "models = {}\n",
    "for model_name,model in zip(\n",
    "    ['NB'],\n",
    "    [NB_pipeline]):\n",
    "    for cat_idx, category in enumerate(classes):\n",
    "        print('... Processing {}'.format(category))\n",
    "        # train the model using X_dtm & y\n",
    "        model.fit(body_train, train_tags[:,cat_idx])\n",
    "        # compute the testing accuracy\n",
    "        prediction = model.predict(body_test)\n",
    "        print(f'{model_name}')\n",
    "        print('    Test accuracy is {}'.format(accuracy_score(test_tags[:, cat_idx], prediction)))\n",
    "        models[category] = model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-27T09:24:16.670320Z",
     "start_time": "2021-06-27T09:24:16.668086Z"
    }
   },
   "outputs": [],
   "source": [
    "# # Prediction\n",
    "# # TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
